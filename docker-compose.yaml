services:
  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    networks:
      - openwebui
    ports:
      - "8080:8080"
    volumes:
      - ./searxng-config/settings.yml:/etc/searxng/settings.yml:ro
    environment:
      - BASE_URL=http://localhost:8080/
      - INSTANCE_NAME=openwebui-search
      - SEARXNG_SECRET=your-secret-key-here
      - SEARXNG_HOSTNAME=0.0.0.0
    restart: unless-stopped

  pipelines:
    build:
      context: ./pipelines
      dockerfile: Dockerfile
      args:
        MINIMUM_BUILD: "false"
        USE_CUDA: "false"
    container_name: pipelines
    volumes:
      - ./pipelines/pipelines:/app/pipelines:rw
    ports:
      - "9099:9099"
    environment:
      - HOST=0.0.0.0
      - PORT=9099
      - API_KEY=0p3n-w3bu!
      - DEBUG=True
      - PIPELINES_DIR=/app/pipelines
    networks:
      - openwebui
    restart: unless-stopped

  open-webui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
      - ./uploads:/app/backend/uploads:rw
      - ./storage:/app/backend/storage:rw
      - ./configs/tts:/app/configs/tts:ro
    ports:
      - "3000:8080"
    environment:
      # Point to local Ollama instance
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - SEARXNG_QUERY_URL=http://searxng:8080/search
      # RAG and file handling settings
      - ENABLE_RAG=true
      - ENABLE_RAG_WEB_SEARCH=true
      - RAG_WEB_SEARCH_ENGINE=searxng
      - RAG_EMBEDDING_MODEL=all-MiniLM-L6-v2
      - RAG_CHUNK_SIZE=1000
      - RAG_CHUNK_OVERLAP=100
      - RAG_TOP_K=3
      - RAG_FILE_MAX_SIZE=50000000
      - RAG_FILE_MAX_COUNT=10
      # File upload settings
      - FILE_UPLOAD_PATH=/app/backend/uploads
      - STORAGE_PATH=/app/backend/storage
      # TTS settings
      - ENABLE_TTS=true
      - USE_BROWSER_TTS=false
      - TTS_SERVICES_CONFIG_DIR=/app/configs/tts
      # For OpenAI/openedai-speech
      - TTS_PROVIDER=openai
      - TTS_OPENAI_API_BASE_URL=http://openedai-speech:8000/v1
      - TTS_OPENAI_API_KEY=sk-111111111
      - TTS_OPENAI_MODEL=tts-1
      - TTS_OPENAI_VOICE=alloy
      # OpenAI API integration
      - ENABLE_OPENAI_API=true
      - OPENAI_API_BASE_URLS=http://openedai-speech:8000/v1
      - OPENAI_API_KEYS=sk-111111111
      # Audio settings
      - AUDIO_FORMAT=wav
      - AUDIO_SAMPLE_RATE=24000
      - AUDIO_CHANNELS=1
      # Debug settings
      - LOG_LEVEL=debug
      - DEBUG=true
      # Audio settings - Add these explicitly
      - AUDIO_TTS_ENGINE=openai
      - AUDIO_TTS_OPENAI_API_BASE_URL=http://openedai-speech:8000/v1
      - AUDIO_TTS_OPENAI_API_KEY=sk-111111111
      - AUDIO_TTS_MODEL=tts-1
      - AUDIO_TTS_VOICE=alloy
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - openwebui
    depends_on:
      - pipelines
      - searxng
      - openedai-speech

  openedai-speech:
    image: ghcr.io/matatonic/openedai-speech
    container_name: openedai-speech
    environment:
      - 'TTS_HOME=/app/voices'
      - 'HF_HOME=/app/voices'
      - 'ENABLE_HD_MODELS=true'
      - 'DEBUG=true'
      - 'COQUI_TOS_AGREED=1'
      - 'MODELS=tts-1'
      - 'VOICES=alloy,echo,fable,onyx,nova,shimmer,american_male,british_male'
      - 'SAMPLE_RATE=24000'
      - 'AUDIO_FORMAT=wav'
      - 'ENABLE_TRANSFORMERS=true'
      - 'API_KEY=sk-111111111'
      - 'API_BASE=http://localhost:8000/v1'
    ports:
      - "8087:8000"
    volumes:
      - openedai-voices:/app/voices
      - openedai-config:/app/config
      - ./data/openedai-speech/config/pre_process_map.default.yaml:/app/config/pre_process_map.default.yaml:ro
      - ./data/openedai-speech/config/voice_to_speaker.default.yaml:/app/config/voice_to_speaker.default.yaml:ro
    networks:
      - openwebui
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  openwebui:
    driver: bridge

volumes:
  openedai-voices: {}
  openedai-config: {}
  open-webui: {}
